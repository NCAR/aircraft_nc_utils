import json
from logging import config
import traceback
import xarray as xr
import numpy as np
import pandas as pd
from datetime import datetime
import os 

class NetCDFtoICARTT:
    """
    A class to convert specific NetCDF aerosol size distribution data
    to the ICARTT FFI 2110 format.
    """
    def __init__(self, netcdf_file_path, output_icartt_file_path):
        """
        Initializes the converter with input and output file paths.
        
        Args:
            netcdf_file_path (str): Path to the input NetCDF file.
            output_icartt_file_path (str): Path for the output ICARTT file.
        """
        self.netcdf_file_path = netcdf_file_path
        self.output_icartt_file_path = output_icartt_file_path
        self.ds = None
        self.time_data = None
        self.uhsas_midpoint_diameters = None
        self.uhsas_bounds = None
        self.auhsas_ro_data = None
        self.num_time_points = 0
        self.num_uhsas_bins = 0
        self.missing_value_primary = int(-99999)
        self.flight_date_utc = None
        

    def _load_data(self):
        """Loads and validates data from the NetCDF file."""
        try:
            self.ds = xr.open_dataset(self.netcdf_file_path, decode_times=False)
        except FileNotFoundError:
            print(f"Error: NetCDF file not found at {self.netcdf_file_path}")
            return False
        except Exception as e:
            print(f"Error opening NetCDF file: {e}")
            return False

        try:
            self.time_data = self.ds['Time'].values
            self.uhsas_midpoint_diameters = self.ds['UHSAS059'].values
            self.uhsas_bounds = self.ds['UHSAS059_bnds'].values
            self.auhsas_ro_data = self.ds['AUHSAS_RO'].squeeze().values
            
            self.num_time_points = len(self.time_data)
            self.num_uhsas_bins = len(self.uhsas_midpoint_diameters)
            
            flight_date_str = self.ds.attrs['FlightDate']
            # Adjust the format string below to match the actual format of flight_date_str, e.g. "%Y-%m-%d"
            self.flight_date_utc = datetime.strptime(flight_date_str, "%d/%m/%Y").strftime('%Y, %m, %d')
            self.platform = "C130" if self.ds.attrs.get('platform') == 'N130AR' else "GV" if self.ds.attrs.get('platform') == 'N1677F' else "Unknown"
            self.project_name = self.ds.attrs.get('project', 'Unknown')
            self.replace_dict = {"DATA_DATE": self.flight_date_utc,
                                "REVISION_DATE": datetime.today().strftime('%Y, %m, %d'),
                                "PLATFORM": self.platform,
                                "PROJECT_NAME": self.project_name}

        except KeyError as e:
            print(f"Error: Missing expected variable or dimension in NetCDF: {e}")
            print("Please ensure your NetCDF file contains 'Time', 'UHSAS059', 'UHSAS059_bnds', and 'AUHSAS_RO'.")
            return False
        
        return True
    
    def _generate_header_base(self, config):
        """
        Helper function to generate the base header content from a template
        and a configuration dictionary.
        """
        lib_path = os.path.dirname(os.path.abspath(__file__))
        template_path = os.path.join(lib_path, 'lib/icartt_template.txt')

        if not os.path.exists(template_path):
            print(f"Error: Header template file not found at {template_path}")
            return None

        try:
            with open(template_path, 'r') as f:
                header_content = f.read()
            # Handle special date formatting
            for key in self.replace_dict:
                header_content = header_content.replace(f"<{key.upper()}>", str(self.replace_dict[key]))
            # First, process header section keys
            if 'header' in config:
                for key, value in config['header'].items():
                    print("Processing header key:", key)
                    placeholder = f"<{key.upper()}>"
                    # Replace the placeholder in header content
                    if placeholder in header_content:
                        header_content = header_content.replace(placeholder, str(value))
                        print(f"Replaced {placeholder} with: {value}")
            print(header_content)
            # Then, process normal_comments section
            if 'normal_comments' in config:
                # Count normal comments first
                normal_comment_count = len(config['normal_comments'])+len(config['normal_comments']['revision_comments'])-1
                header_content = header_content.replace("<N_NORMAL_COMMENTS>", str(normal_comment_count))

                for key, value in config['normal_comments'].items():
                    placeholder = f"<{key.upper()}>"

                    # Handle special cases for normal comments
                    if key.lower() == "revision_comments":
                        # Add each revision comment as a new line
                        for version, comment in value.items():
                            header_content += f"\n{version}: {comment}"
                        continue  # Skip the default placeholder replacement for this key
                    # Replace the placeholder in header content
                    if placeholder in header_content:
                        header_content = header_content.replace(placeholder, str(value))
                        print(f"Replaced {placeholder} with: {value}")

            # Handle special comments count (if exists)
            if 'special_comments' in config:
                special_comment_count = len(config['special_comments']) if config['special_comments'] else 0
                header_content = header_content.replace("<N_SPECIAL_COMMENTS>", str(special_comment_count))
            else:
                header_content = header_content.replace("<N_SPECIAL_COMMENTS>", "0")

            # Handle any remaining placeholders that might be computed values
            header_content = header_content.replace("<INDEPENDENT_VARIABLE>", "Time_Start")

            return header_content

        except Exception as e:
            print(f"Error processing template: {e}")
            print(traceback.format_exc())
            return None
    def ICARTTHeader(self, config_file=None):
        """
        Generates a customizable ICARTT header for FFI 1001 format (3D data as columns).
        """
        config = self._load_or_create_config(config_file)
        if not config:
            return None

        header_content = self._generate_header_base(config)
        if not header_content:
            return None

        try:
            # Logic specific to FFI 1001 format
            num_vars = 2 + self.num_uhsas_bins
            header_content = header_content.replace("<N_VARIABLES>", str(num_vars))
            
            fill_values_str = ",".join([str(config.get('missing_value', self.missing_value_primary))] * num_vars)
            header_content = header_content.replace("<FILL_VALUES>", fill_values_str)

            var_list = ["UTC", "NumBins", *[f"UHSAS_BIN_{i+1}" for i in range(self.num_uhsas_bins)]]
            header_content = header_content.replace("<VARIABLE_LIST>", ", ".join(var_list))

            header_lines = header_content.strip().split('\n')
            nlhead, final_lines = self._cleanup_header(header_lines)
            header_content = f"{nlhead}, 1001\n" + "\n".join(final_lines[1:])

            return header_content
            
        except Exception:
            print(traceback.format_exc())
            return None
    def _cleanup_header(self, header_lines):
        final_lines = []
        for line in header_lines:
            if '<' in line and '>' in line:
                print(f"⚠️ Warning: Removing extraneous header line with unresolved placeholder: '{line}'")
            else:
                final_lines.append(line)

        nlhead = len(final_lines)
        return nlhead, final_lines
    def generate_header(self, config_file=None):
        """
        Generates a customizable ICARTT header for the original data structure.
        This corresponds to FFI 2110.
        """
        #read json config file
        with open(config_file, 'r') as f:
            config = json.load(f)
        if not config:
            return None
        print(config)
        header_content = self._generate_header_base(config)
        if not header_content:
            return None

        try:
            # Logic specific to FFI 2110 format
            header_lines = header_content.strip().split('\n')
            # Add specific FFI 2110 lines that are not in the generic template
            header_lines.append("1.0")
            header_lines.append("UHSAS059[], um, arithmetic midpoint bin size in diameter")
            header_lines.append("UTC, seconds since midnight on flight_date")
            header_lines.append(str(1))
            header_lines.append("1.0")
            header_lines.append(str(self.missing_value_primary))
            header_lines.append("AUHSAS_RO[], count, UHSAS Raw Count Histogram")

            aux_vars_list = [{"name": "NumBins", "units": "none", "description": "Number_of_UHSAS_bins_reported"}]
            header_lines.append(str(len(aux_vars_list)))
            header_lines.append(", ".join([str(1.0) for _ in aux_vars_list]))
            header_lines.append(", ".join([str(self.missing_value_primary) for _ in aux_vars_list]))
            for aux_var in aux_vars_list:
                header_lines.append(f"{aux_var['name']}, {aux_var['units']}, {aux_var['description']}")            
            
            # Add data column header
            data_column_header_parts = ["UTC"]
            data_column_header_parts.append("NumBins")
            data_column_header_parts.append("UHSAS059[]")
            data_column_header_parts.append("AUHSAS_RO[]")
            header_lines.append(", ".join(data_column_header_parts))
            # Placeholder removal and warning logic
            nlhead, final_lines = self._cleanup_header(header_lines)
            final_lines[0] = f"{nlhead}, 2110"
            
            return "\n".join(final_lines)
        except Exception:
            print(traceback.format_exc())
            return None

    def _generate_data_rows(self):
        """Generates the ICARTT data rows."""
        data_rows = []
        for i in range(self.num_time_points):
            current_utc = self.time_data[i]
            current_auhsas_ro_array = self.auhsas_ro_data[i, :]

            main_data_line_parts = [f"{current_utc:.0f}"]
            main_data_line_parts.append(str(self.num_uhsas_bins))
            data_rows.append(", ".join(main_data_line_parts))

            for j in range(self.num_uhsas_bins):
                uhsas_diameter_val = self.uhsas_midpoint_diameters[j]
                auhsas_ro_val = current_auhsas_ro_array[j]
                if np.isnan(auhsas_ro_val):
                    auhsas_ro_val = self.missing_value_primary
                
                data_rows.append(f"   {uhsas_diameter_val:.6f}, {auhsas_ro_val:.0f}")
        return data_rows

    def convert(self):
        """Performs the full conversion from NetCDF to ICARTT."""
        if not self._load_data():
            return

        #header_lines = self.ICARTTHeader('../icartt.json')
        header_lines = self.generate_header('icartt.json')
        print(header_lines)
        data_rows = self._generate_data_rows()

        with open(self.output_icartt_file_path, 'w') as f:
            f.writelines(header_lines + '\n')
            for row in data_rows:
                f.write(row + "\n")
        print(f"Successfully converted NetCDF to ICARTT FFI 2110: {self.output_icartt_file_path}")

# Example of how to use the new class
if __name__ == "__main__":
    # Create a dummy NetCDF file reflecting the specified structure
    input_netcdf = "/Users/srunkel/Downloads/GOTHAAMrf08.nc"
    output_icartt_path = "GOTHAAM-CORE_C130_20250804_R0.ict"
    # Instantiate the class and call the convert method
    converter = NetCDFtoICARTT(input_netcdf, output_icartt_path)
    converter.convert()